{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center> Unidad 7: Web Scraping</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.03001,
     "end_time": "2020-11-10T13:18:37.840723",
     "exception": false,
     "start_time": "2020-11-10T13:18:37.810713",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Introducci√≥n\n",
    "En esta lecci√≥n revisaremos el proceso de **Web Scraping**. El **Web Scraping**, tambi√©n conocido como raspado de p√°ginas web, es un proceso automatizado que permite extraer informaci√≥n de p√°ginas web y almacenarla en un formato estructurado para su posterior an√°lisis. Esta t√©cnica se utiliza com√∫nmente para recopilar grandes cantidades de datos de manera eficiente y automatizada, y se utiliza en diversas aplicaciones como la monitorizaci√≥n de precios, an√°lisis de mercado, investigaci√≥n de la competencia, entre otras.\n",
    "\n",
    "B√°sicamente, el web scraping implica el uso de software programado, como bots, crawlers o spiders, para rastrear uno o varios sitios web y extraer autom√°ticamente la informaci√≥n, los contenidos y otros datos que contienen. El proceso es comparable al copiado y pegado, pero se realiza de manera automatizada y a gran escala.La mayor√≠a de los datos extra√≠dos mediante el web scraping son no estructurados, es decir, en formato HTML, y necesitan ser transformados en datos estructurados para su uso posterior en aplicaciones. Esto se puede lograr mediante la conversi√≥n de los datos extra√≠dos en una hoja de c√°lculo o una base de datos, permitiendo as√≠ el an√°lisis y la toma de decisiones basada en datos.\n",
    "\n",
    "Entre las aplicaciones m√°s comunes del web scraping se encuentra la monitorizaci√≥n de precios en l√≠nea, que utiliza bots para extraer informaci√≥n sobre precios de diferentes sitios web, permitiendo as√≠ a las empresas comparar precios y ofrecer productos y servicios competitivos. Otras aplicaciones incluyen la investigaci√≥n de la competencia, an√°lisis de mercado, detecci√≥n de fraudes, entre otras.\n",
    "\n",
    "En resumen, el web scraping es una t√©cnica automatizada para la recopilaci√≥n de datos de sitios web, que se utiliza com√∫nmente en diversas aplicaciones comerciales y de investigaci√≥n. Permite la extracci√≥n de grandes cantidades de informaci√≥n de manera eficiente y automatizada, lo que lo hace especialmente √∫til para la toma de decisiones basada en datos.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.03001,
     "end_time": "2020-11-10T13:18:37.840723",
     "exception": false,
     "start_time": "2020-11-10T13:18:37.810713",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Caso pr√°ctico\n",
    "\n",
    "Este material expande el marco inicial de las t√©cnicas de web scraping y aborda la tem√°tica en un contexto del an√°lisis y resoluci√≥n de un caso pr√°ctico de an√°lisis de sentimientos con Python. Tiene por objetivo analizar la aplicabilidad de los m√©todos de web scraping para el an√°lisis de sentimientos usando como referencia las 9 etapas del ciclo de vida del Big Data.\n",
    "\n",
    "**Que revisaremos?**\n",
    "* C√≥mo usar web scraping para obtener rese√±as de pel√≠culas de IMDB\n",
    "* C√≥mo limpiar los datos\n",
    "* Qu√© es el An√°lisis de Sentimientos\n",
    "* C√≥mo usar t√©cnicas de procesamiento del lenguaje natural (NLP)\n",
    "\n",
    "üí° _**Nota**_: _Este material ha sido preparado por la **Universidad de las Am√©ricas - UDLA**. Parte del contenido ha sido adaptado a partir de material p√∫blico liberado en [Kaggle](kaggle.com), [Datacamp](datacamp.com) y otros sitios web._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sobre la empresa\n",
    "\n",
    "El siguiente caso de negocio es tomado de **Internet Movie Database (IMDb, en espa√±ol: Base de datos de pel√≠culas en Internet)** es una base de datos en l√≠nea que en un principio almacena informaci√≥n relacionada con pel√≠culas, y con el tiempo se transforma en la base de datos m√°s grande del mundo donde se encuentran programas de televisi√≥n, eventos en vivo y difundidos en televisi√≥n o en la web, entrega de premios y especiales. Se encuentra el personal de equipo de producci√≥n (incluyendo directores y productores), actores, series y programas de televisi√≥n, videojuegos, actores de doblaje y personajes ficticios que aparecen en los medios de entretenimiento visual. Recibe m√°s de 100 millones de usuarios √∫nicos al mes y cuenta con una versi√≥n m√≥vil. IMDb fue inaugurada el 17 de octubre de 1985 y en 1998 fue adquirida por [Amazon](https://www.amazon.com).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.03001,
     "end_time": "2020-11-10T13:18:37.840723",
     "exception": false,
     "start_time": "2020-11-10T13:18:37.810713",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## 1. Evaluaci√≥n de necesidades del negocio\n",
    "La fase de evaluaci√≥n del caso de negocio requiere que se cree, eval√∫e y apruebe un caso de negocio antes de proceder a las tareas reales de an√°lisis pr√°ctico.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En 1996, **IMDb** se constituy√≥ como empresa en el Reino Unido, con el nombre de The Internet Movie Database Ltd. El fundador, Col Needham, se convirti√≥ en el propietario primario as√≠ como en su imagen principal. IMDb se convirti√≥ en un servicio patrocinado y financiado con publicidad. El debut empresarial se acompa√±√≥ de nuevas secciones y una reescritura y optimizaci√≥n del sitio web.\n",
    "\n",
    "Los antecedentes de **IMDb** se remontan a 1989, cuando un aficionado public√≥ una discusi√≥n en un grupo de noticias en usenet sobre actrices atractivas. A partir de ese momento, otros usuarios de la lista comenzaron a recopilar actores y actrices con las pel√≠culas en las que hab√≠an intervenido. La base de datos original fue construida a partir de las listas de cr√©ditos que Col Needham y otros dos lectores, Dave Knight y Andy Krieg, hab√≠an comenzado a publicar en el grupo. Needham public√≥ en septiembre de 1991 la primera herramienta simple (una serie de shell scripts de Unix) que permit√≠a la consulta de las listas existentes en ese momento: la lista de directores, la de actores, la de actrices y la de actores fallecidos. La fusi√≥n de las cuatro la base de datos resultante se convertir√≠a finalmente en la **IMDb**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.03001,
     "end_time": "2020-11-10T13:18:37.840723",
     "exception": false,
     "start_time": "2020-11-10T13:18:37.810713",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 3. Adquisici√≥n y filtrado de datos\n",
    "Durante la etapa de adquisici√≥n y filtrado de datos se recopilan los datos de todas las fuentes de datos que se identificaron durante la etapa Identificaci√≥n de datos.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.03001,
     "end_time": "2020-11-10T13:18:37.840723",
     "exception": false,
     "start_time": "2020-11-10T13:18:37.810713",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Dataset** \n",
    "\n",
    "Este conjunto de datos contiene las rese√±as de pel√≠culas de **IMDb**. Se trata de un conjunto de datos para la clasificaci√≥n binaria (comentarios positivos y negativos) de sentimientos y contiene alrededor de 4.000 cr√≠ticas de pel√≠culas muy populares."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.03001,
     "end_time": "2020-11-10T13:18:37.840723",
     "exception": false,
     "start_time": "2020-11-10T13:18:37.810713",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**Diccionario de datos**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* `Movie_ID`: Id de la pelicula.\n",
    "* `Review`: Rese√±a pel√≠cula.\n",
    "* `Rating`: Calificaci√≥n de la pelicula."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.03001,
     "end_time": "2020-11-10T13:18:37.840723",
     "exception": false,
     "start_time": "2020-11-10T13:18:37.810713",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 4. Extracci√≥n de datos\n",
    "Durante esta etapa se debe extraer datos dispares y transformarlos en un formato que pueda facilitar el an√°lisis de datos.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.035782,
     "end_time": "2020-11-10T13:18:38.409834",
     "exception": false,
     "start_time": "2020-11-10T13:18:38.374052",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "El \"scraper\" primero hace clic en el bot√≥n para cargar todas las rese√±as. Cuando carga todas las rese√±as, busca los elementos de rese√±a y valoraci√≥n, los almacena en una cadena y la a√±ade a la lista. M√°s tarde convertimos la lista en un marco de datos y lo guardamos en formato csv."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importar librer√≠as y m√≥dulos espec√≠ficos de librer√≠as**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_kg_hide-input": true,
    "_kg_hide-output": true,
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "papermill": {
     "duration": 0.044555,
     "end_time": "2020-11-10T13:18:37.915836",
     "exception": false,
     "start_time": "2020-11-10T13:18:37.871281",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cargar el dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"https://raw.githubusercontent.com/vhteran/UDLA_Big_Data_aplicada_a_los_Negocios/main/data/get_out_cleaned.csv\"\n",
    "df = pd.read_csv(url, sep = \";\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.03001,
     "end_time": "2020-11-10T13:18:37.840723",
     "exception": false,
     "start_time": "2020-11-10T13:18:37.810713",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 5. Validaci√≥n y limpieza de datos\n",
    "Los datos no v√°lidos pueden sesgar y falsear los resultados de los an√°lisis. La etapa de validaci√≥n y depuraci√≥n de datos se enfoca en establecer reglas de validaci√≥n a menudo complejas y a eliminar cualquier dato no v√°lido conocido.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>,Movie_ID,Review,Rating,Rate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1,1,\" interesting movie about an interracial c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the idea is original and interesting the actin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the movie has its share of racism that cant go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>out of  found this helpful was this review he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>permalink\",positive,9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4205</th>\n",
       "      <td>out of  found this helpful was this review he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4206</th>\n",
       "      <td>permalink\",positive,6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4207</th>\n",
       "      <td>1527,1527,\" i like how different yet compellin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4208</th>\n",
       "      <td>out of  found this helpful was this review he...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4209</th>\n",
       "      <td>permalink\",positive,6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4210 rows √ó 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           ,Movie_ID,Review,Rating,Rate\n",
       "0     1,1,\" interesting movie about an interracial c...\n",
       "1     the idea is original and interesting the actin...\n",
       "2     the movie has its share of racism that cant go...\n",
       "3      out of  found this helpful was this review he...\n",
       "4                                 permalink\",positive,9\n",
       "...                                                 ...\n",
       "4205   out of  found this helpful was this review he...\n",
       "4206                              permalink\",positive,6\n",
       "4207  1527,1527,\" i like how different yet compellin...\n",
       "4208   out of  found this helpful was this review he...\n",
       "4209                              permalink\",positive,6\n",
       "\n",
       "[4210 rows x 1 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Visualizamos el dataset\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.032081,
     "end_time": "2020-11-10T13:18:38.641806",
     "exception": false,
     "start_time": "2020-11-10T13:18:38.609725",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "En primer lugar, debemos cambiar el nombre de la columna \"Sin nombre: 0\" a \"Movie_ID\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-10T13:18:38.713410Z",
     "iopub.status.busy": "2020-11-10T13:18:38.712266Z",
     "iopub.status.idle": "2020-11-10T13:18:38.715855Z",
     "shell.execute_reply": "2020-11-10T13:18:38.715175Z"
    },
    "papermill": {
     "duration": 0.041841,
     "end_time": "2020-11-10T13:18:38.715977",
     "exception": false,
     "start_time": "2020-11-10T13:18:38.674136",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.rename(columns={\"Unnamed: 0\" : \"Movie_ID\"},inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.032748,
     "end_time": "2020-11-10T13:18:38.781146",
     "exception": false,
     "start_time": "2020-11-10T13:18:38.748398",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "Ahora, dividiremos la columna \"Valoraci√≥n\" en \"|\" y almacenaremos la valoraci√≥n en una nueva columna llamada \"Valoraci√≥n\". Asi, la columna \"Valoraci√≥n\" s√≥lo tendr√° el puntaje."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-10T13:18:38.866216Z",
     "iopub.status.busy": "2020-11-10T13:18:38.865315Z",
     "iopub.status.idle": "2020-11-10T13:18:38.868813Z",
     "shell.execute_reply": "2020-11-10T13:18:38.869588Z"
    },
    "papermill": {
     "duration": 0.055117,
     "end_time": "2020-11-10T13:18:38.869783",
     "exception": false,
     "start_time": "2020-11-10T13:18:38.814666",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Review'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3801\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Review'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRating\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mReview\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m|\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m      2\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReview\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReview\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m|\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(df\u001b[38;5;241m.\u001b[39mhead())\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3807\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3805\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3807\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3809\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3804\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3805\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3806\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3808\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3809\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Review'"
     ]
    }
   ],
   "source": [
    "df[\"Rating\"] = df[\"Review\"].apply(lambda x: x.split(\"|\")[0])\n",
    "df[\"Review\"] = df[\"Review\"].apply(lambda x: x.split(\"|\")[1])\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.032727,
     "end_time": "2020-11-10T13:18:38.936186",
     "exception": false,
     "start_time": "2020-11-10T13:18:38.903459",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Ahora nos centraremos en limpiar las columnas de \"Revisi√≥n\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-10T13:18:39.020372Z",
     "iopub.status.busy": "2020-11-10T13:18:39.019546Z",
     "iopub.status.idle": "2020-11-10T13:18:39.182996Z",
     "shell.execute_reply": "2020-11-10T13:18:39.182208Z"
    },
    "papermill": {
     "duration": 0.213498,
     "end_time": "2020-11-10T13:18:39.183119",
     "exception": false,
     "start_time": "2020-11-10T13:18:38.969621",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Review'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3801\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\_libs\\index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Review'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mReview\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m, np\u001b[38;5;241m.\u001b[39mnan, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;66;03m# replacing space with nan\u001b[39;00m\n\u001b[0;32m      3\u001b[0m df\u001b[38;5;241m.\u001b[39mdropna(inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;66;03m#drop nan values, coz some of review didn't have actual review\u001b[39;00m\n\u001b[0;32m      4\u001b[0m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReview\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReview\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mstr\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;66;03m# Converting everything to lower coz \"Movie\" and \"movie\" is not the same thing\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\frame.py:3807\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3805\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3806\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3807\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   3808\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3809\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py:3804\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m   3802\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m-> 3804\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3805\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3806\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3807\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3808\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3809\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Review'"
     ]
    }
   ],
   "source": [
    "df['Review'].replace(\" \", np.nan, inplace=True) # replacing space with nan\n",
    "\n",
    "df.dropna(inplace=True) #drop nan values, coz some of review didn't have actual review\n",
    "df[\"Review\"] = df[\"Review\"].str.lower() # Converting everything to lower coz \"Movie\" and \"movie\" is not the same thing\n",
    "\n",
    "# Removing special characters from Reviews\n",
    "spec_chars = [\"¬±\",\"@\",\"#\",\"$\",\"%\",\"^\",\n",
    "                 \"&\",\"*\",\"(\",\")\",\"_\",\"+\",\"=\",\n",
    "                 \"-\",\"/\",\">\",\"<\",\"?\",\n",
    "                 \"~\",\"`\",\"'\",\"[\",\"]\",\"|\",\"}\",\n",
    "                 \"{\",'\"', \".\",\",\",\"!\",\";\"]\n",
    "\n",
    "for char in spec_chars:\n",
    "    df[\"Review\"] = df[\"Review\"].str.replace(char, \"\")\n",
    "\n",
    "df[\"Review\"].apply(lambda x: x.encode('ascii', 'ignore').decode('ascii')) # getting rid of emojis\n",
    "\n",
    "df['Review'] = df['Review'].str.replace('\\d+', '') # Remove numbers from Reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.033145,
     "end_time": "2020-11-10T13:18:39.250233",
     "exception": false,
     "start_time": "2020-11-10T13:18:39.217088",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Ha llegado el momento de convertir la columna de cr√≠ticas en cr√≠ticas Positivas y Negativas. Nos desharemos de la barra y del n√∫mero diez. As√≠, \"N/10\" queremos guardar la N porque en realidad es la valoraci√≥n dejada por el usuario. A continuaci√≥n convertiremos la columna en num√©rica y utilizaremos el bucle for para convertir las valoraciones menores de 5 en negativas y las mayores o iguales de 5 en positivas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_kg_hide-output": false,
    "execution": {
     "iopub.execute_input": "2020-11-10T13:18:39.330085Z",
     "iopub.status.busy": "2020-11-10T13:18:39.329104Z",
     "iopub.status.idle": "2020-11-10T13:18:39.951209Z",
     "shell.execute_reply": "2020-11-10T13:18:39.950429Z"
    },
    "papermill": {
     "duration": 0.667893,
     "end_time": "2020-11-10T13:18:39.951380",
     "exception": false,
     "start_time": "2020-11-10T13:18:39.283487",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df[\"Rate\"] = df[\"Rating\"].apply(lambda x: x.split(\"/\")[0])\n",
    "df[\"Rate\"] = pd.to_numeric(df[\"Rate\"])\n",
    "\n",
    "for i in range(len(df)):\n",
    "    if (df[\"Rate\"].iloc[i] < 5):\n",
    "        df[\"Rating\"].iloc[i] = \"negative\"\n",
    "    else:\n",
    "        df[\"Rating\"].iloc[i] = \"positive\"\n",
    "        \n",
    "df.to_csv(\"get_out_cleaned.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.03001,
     "end_time": "2020-11-10T13:18:37.840723",
     "exception": false,
     "start_time": "2020-11-10T13:18:37.810713",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 6. Agregaci√≥n y representaci√≥n de datos\n",
    "La etapa de agregaci√≥n y representaci√≥n de datos, se dedica a integrar m√∫ltiples conjuntos de datos para obtener una visi√≥n unificada.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.03001,
     "end_time": "2020-11-10T13:18:37.840723",
     "exception": false,
     "start_time": "2020-11-10T13:18:37.810713",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Este caso de uso no requiere la integraci√≥n de otros datos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.03001,
     "end_time": "2020-11-10T13:18:37.840723",
     "exception": false,
     "start_time": "2020-11-10T13:18:37.810713",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 7. An√°lisis de datos\n",
    "La etapa de an√°lisis de datos se enfoca en llevar a cabo la tarea de an√°lisis propiamente dicha, que suele implicar uno o m√°s tipos de an√°lisis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.033562,
     "end_time": "2020-11-10T13:18:40.019067",
     "exception": false,
     "start_time": "2020-11-10T13:18:39.985505",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <a id='Sentiment-Analysis' style=\"color:black\">**An√°lisis de Sentimientos**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.033514,
     "end_time": "2020-11-10T13:18:40.154361",
     "exception": false,
     "start_time": "2020-11-10T13:18:40.120847",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "\n",
    "El sistema de an√°lisis de sentimientos para el an√°lisis de textos combina t√©cnicas de procesamiento del lenguaje natural (PLN) y de aprendizaje autom√°tico para asignar puntuaciones ponderadas de sentimientos a las entidades, temas y categor√≠as de una frase u oraci√≥n. Utilizar el an√°lisis de sentimientos podr√≠a ayudarle a descubrir si los clientes est√°n satisfechos con sus planes de precios, su servicio de atenci√≥n al cliente, etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.033453,
     "end_time": "2020-11-10T13:18:40.221678",
     "exception": false,
     "start_time": "2020-11-10T13:18:40.188225",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Importemos las bibliotecas y carguemos el marco de datos limpiado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-10T13:18:40.299790Z",
     "iopub.status.busy": "2020-11-10T13:18:40.298983Z",
     "iopub.status.idle": "2020-11-10T13:18:42.329099Z",
     "shell.execute_reply": "2020-11-10T13:18:42.328107Z"
    },
    "papermill": {
     "duration": 2.07381,
     "end_time": "2020-11-10T13:18:42.329267",
     "exception": false,
     "start_time": "2020-11-10T13:18:40.255457",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import ComplementNB\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "\n",
    "df = pd.read_csv('/kaggle/input/movies/get_out_cleaned.csv')\n",
    "df.drop(columns=[\"Unnamed: 0\"],inplace=True)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.03423,
     "end_time": "2020-11-10T13:18:42.398994",
     "exception": false,
     "start_time": "2020-11-10T13:18:42.364764",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    " <a id='Tokenization' style=\"color:black\">**Tokenizaci√≥n**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.034533,
     "end_time": "2020-11-10T13:18:42.468399",
     "exception": false,
     "start_time": "2020-11-10T13:18:42.433866",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "**La tokenizaci√≥n** es el proceso de convertir texto en tokens antes de transformarlo en vectores. Tambi√©n es m√°s f√°cil filtrar los tokens innecesarios. Por ejemplo, un documento en p√°rrafos o frases en palabras. En este caso estamos tokenizando las rese√±as en palabras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-10T13:18:42.554097Z",
     "iopub.status.busy": "2020-11-10T13:18:42.553333Z",
     "iopub.status.idle": "2020-11-10T13:18:44.370713Z",
     "shell.execute_reply": "2020-11-10T13:18:44.370026Z"
    },
    "papermill": {
     "duration": 1.867647,
     "end_time": "2020-11-10T13:18:44.370872",
     "exception": false,
     "start_time": "2020-11-10T13:18:42.503225",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english')) # eng stopwords\n",
    "df['tokenized_reviews'] = df.apply(lambda row: word_tokenize(row['Review']), axis=1) # Tokenization of reviews\n",
    "df[\"filtered\"] = df['tokenized_reviews'].apply(lambda x: [item for item in x if item not in stop_words]) #Removing stopwrods from tokenized data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.038557,
     "end_time": "2020-11-10T13:18:44.444498",
     "exception": false,
     "start_time": "2020-11-10T13:18:44.405941",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id='Count-Vectorizer' style=\"color:black\">**Vectorizador de conteo**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.034483,
     "end_time": "2020-11-10T13:18:44.513725",
     "exception": false,
     "start_time": "2020-11-10T13:18:44.479242",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "El **CountVectorizer** se utiliza para convertir una colecci√≥n de documentos de texto en un vector de recuentos de t√©rminos/tokens.\n",
    "\n",
    "Por ejemplo words=[\"movie\", \"good\", \"perfect, \"good\"] se transformar√≠a en: \"pel√≠cula\" = 1, \"bueno\" = 2, \"perfecto\" = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "papermill": {
     "duration": 0.249821,
     "end_time": "2020-11-10T13:18:44.799058",
     "exception": false,
     "start_time": "2020-11-10T13:18:44.549237",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['filtered']=df['filtered'].apply(str)\n",
    "cv = CountVectorizer(lowercase=False,stop_words='english',binary=True)\n",
    "cv.fit(df[\"filtered\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.035183,
     "end_time": "2020-11-10T13:18:44.869700",
     "exception": false,
     "start_time": "2020-11-10T13:18:44.834517",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Inicialicemos nuestra caracter√≠stica y objetivo y hagamos la divisi√≥n de prueba de entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-10T13:18:44.990277Z",
     "iopub.status.busy": "2020-11-10T13:18:44.969381Z",
     "iopub.status.idle": "2020-11-10T13:18:45.100821Z",
     "shell.execute_reply": "2020-11-10T13:18:45.100084Z"
    },
    "papermill": {
     "duration": 0.196145,
     "end_time": "2020-11-10T13:18:45.100961",
     "exception": false,
     "start_time": "2020-11-10T13:18:44.904816",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = cv.transform(df[\"filtered\"])\n",
    "y = df[\"Rating\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.03001,
     "end_time": "2020-11-10T13:18:37.840723",
     "exception": false,
     "start_time": "2020-11-10T13:18:37.810713",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 8. Visualizaci√≥n de datos\n",
    "La capacidad de analizar grandes cantidades de datos y obtener informaci√≥n √∫til tiene poco valor si los √∫nicos que pueden interpretar los resultados son los analistas.\n",
    "La etapa de visualizaci√≥n de datos se dedica a utilizar t√©cnicas y herramientas de visualizaci√≥n de datos para comunicar gr√°ficamente los resultados del an√°lisis con vistas a una interpretaci√≥n eficaz por parte de los usuarios empresariales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.036798,
     "end_time": "2020-11-10T13:18:45.552186",
     "exception": false,
     "start_time": "2020-11-10T13:18:45.515388",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <a id='KNN' style=\"color:black\">**KNN(K-Nearest-Neighbor)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.036224,
     "end_time": "2020-11-10T13:18:45.624670",
     "exception": false,
     "start_time": "2020-11-10T13:18:45.588446",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "K-Nearest-Neighbor es un algoritmo basado en instancia de tipo supervisado de Machine Learning. Puede usarse para clasificar nuevas muestras (valores discretos) o para predecir (regresi√≥n, valores continuos). Al ser un m√©todo sencillo, es ideal para introducirse en el mundo del  Aprendizaje Autom√°tico. Sirve esencialmente para clasificar valores buscando los puntos de datos ‚Äúm√°s similares‚Äù (por cercan√≠a) aprendidos en la etapa de entrenamiento (ver 7 pasos para crear tu ML) y haciendo conjeturas de nuevos puntos basado en esa clasificaci√≥n.\n",
    "\n",
    "Es un m√©todo que simplemente busca en las observaciones m√°s cercanas a la que se est√° tratando de predecir y clasifica el punto de inter√©s basado en la mayor√≠a de datos que le rodean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.036224,
     "end_time": "2020-11-10T13:18:45.624670",
     "exception": false,
     "start_time": "2020-11-10T13:18:45.588446",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "\n",
    "Elegimos el mejor valor K."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-10T13:18:45.707612Z",
     "iopub.status.busy": "2020-11-10T13:18:45.706801Z",
     "iopub.status.idle": "2020-11-10T13:18:50.323625Z",
     "shell.execute_reply": "2020-11-10T13:18:50.322791Z"
    },
    "papermill": {
     "duration": 4.662293,
     "end_time": "2020-11-10T13:18:50.323786",
     "exception": false,
     "start_time": "2020-11-10T13:18:45.661493",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "errors = []\n",
    "for k in range(1,100):\n",
    "    model = KNeighborsClassifier(n_neighbors=k)\n",
    "    model.fit(X_train,y_train)\n",
    "    pred = model.predict(X_test)\n",
    "    errors.append(np.mean(pred!=y_test))\n",
    "plt.plot(range(1,100),errors)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-10T13:18:50.424735Z",
     "iopub.status.busy": "2020-11-10T13:18:50.423821Z",
     "iopub.status.idle": "2020-11-10T13:18:50.473698Z",
     "shell.execute_reply": "2020-11-10T13:18:50.472881Z"
    },
    "papermill": {
     "duration": 0.108178,
     "end_time": "2020-11-10T13:18:50.473829",
     "exception": false,
     "start_time": "2020-11-10T13:18:50.365651",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=25)\n",
    "knn.fit(X_train,y_train)\n",
    "print(\"K-Nearest Neighbours accuracy score: {}\".format(accuracy_score(y_test, knn.predict(X_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.042796,
     "end_time": "2020-11-10T13:18:50.559717",
     "exception": false,
     "start_time": "2020-11-10T13:18:50.516921",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <a id='Multinomial-Naive-Bayes' style=\"color:black\">**Naive Bayes Multinomial**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-10T13:18:50.651343Z",
     "iopub.status.busy": "2020-11-10T13:18:50.650111Z",
     "iopub.status.idle": "2020-11-10T13:18:50.659246Z",
     "shell.execute_reply": "2020-11-10T13:18:50.658177Z"
    },
    "papermill": {
     "duration": 0.056676,
     "end_time": "2020-11-10T13:18:50.659442",
     "exception": false,
     "start_time": "2020-11-10T13:18:50.602766",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "MNB = MultinomialNB()\n",
    "MNB.fit(X_train,y_train)\n",
    "print(\"Multinomial Naive Bayes accuracy score: {}\".format(accuracy_score(y_test, MNB.predict(X_test))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.038483,
     "end_time": "2020-11-10T13:18:50.911219",
     "exception": false,
     "start_time": "2020-11-10T13:18:50.872736",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# <a id='Word-Cloud' style=\"color:black\">**Nube de palabras**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.038326,
     "end_time": "2020-11-10T13:18:50.988421",
     "exception": false,
     "start_time": "2020-11-10T13:18:50.950095",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Una nube de palabras es una representaci√≥n visual de datos textuales. Muestra una lista de palabras, la importancia de cada una se muestra con el tama√±o de la fuente o el color."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2020-11-10T13:18:51.077105Z",
     "iopub.status.busy": "2020-11-10T13:18:51.076280Z",
     "iopub.status.idle": "2020-11-10T13:18:54.220503Z",
     "shell.execute_reply": "2020-11-10T13:18:54.221574Z"
    },
    "papermill": {
     "duration": 3.194902,
     "end_time": "2020-11-10T13:18:54.221827",
     "exception": false,
     "start_time": "2020-11-10T13:18:51.026925",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "stopwords = set(STOPWORDS)\n",
    "wordcloud = WordCloud(width = 800, height = 800,\n",
    "                      background_color ='white',\n",
    "                      stopwords = stopwords,\n",
    "                      min_font_size = 10).generate(' '.join(df['filtered']))\n",
    "\n",
    "plt.figure(figsize = (8, 8), facecolor = None)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad = 0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.03001,
     "end_time": "2020-11-10T13:18:37.840723",
     "exception": false,
     "start_time": "2020-11-10T13:18:37.810713",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# 9. Utilizaci√≥n de los resultados del an√°lisis\n",
    "Despu√©s de que los resultados del an√°lisis se pongan a disposici√≥n de los usuarios de negocio para apoyar la toma de decisiones empresariales, por ejemplo a trav√©s de cuadros de mando o paneles, puede haber m√°s oportunidades para utilizar los resultados del an√°lisis. La etapa de utilizaci√≥n de los resultados del an√°lisis, esta enfocada en determinar c√≥mo y d√≥nde se pueden aprovechar m√°s los datos del an√°lisis procesado.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.047014,
     "end_time": "2020-11-10T13:18:54.319148",
     "exception": false,
     "start_time": "2020-11-10T13:18:54.272134",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id='Conclusion' style=\"color:black\">**Conclusiones**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.052865,
     "end_time": "2020-11-10T13:18:54.420501",
     "exception": false,
     "start_time": "2020-11-10T13:18:54.367636",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "- Los modelos, con una precisi√≥n media del 78% en un conjunto de datos de unos 1.600, demostraron ser mejores. El modelo funcionar√≠a a√∫n mejor en un conjunto de datos mayor.\n",
    "\n",
    "- Este proyecto puede mejorarse a√±adiendo XGBoost, ¬°y tambi√©n ser√≠a interesante probar TextBlob!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "papermill": {
   "duration": 23.133858,
   "end_time": "2020-11-10T13:18:55.547544",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2020-11-10T13:18:32.413686",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
